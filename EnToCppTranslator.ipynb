{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aabedb39-e7e1-4b97-a0e2-20a2b74af77e",
   "metadata": {},
   "source": [
    "Data Collection/Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dcaffc2-8f91-4173-9399-1a383cefbb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b332b6d-b906-485c-af22-f12ceb29b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Fsoft-AIC/the-vault-function\", split_set=[\"train/small\"], languages=['cpp'], trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcc125b4-ba80-4241-bf8d-66f03f59aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_samples_large = dataset['train_small']['code']\n",
    "docstring_samples_large = dataset['train_small']['short_docstring']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d5b3044-54d4-4948-add8-40d89e82261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset is to large for my computer to handle :(\n",
    "# Define the size of the reduced dataset\n",
    "import random\n",
    "\n",
    "reduced_size = int(len(code_samples_large) * 0.05)\n",
    "\n",
    "# Create a list of indices for the samples in the original dataset\n",
    "indices = list(range(len(code_samples_large)))\n",
    "\n",
    "# Randomly shuffle the indices\n",
    "random.shuffle(indices)\n",
    "\n",
    "# Select the first 75% of the shuffled indices\n",
    "selected_indices = indices[:reduced_size]\n",
    "\n",
    "# Create the reduced dataset using the selected indices\n",
    "code_samples = [code_samples_large[i] for i in selected_indices]\n",
    "docstring_samples = [docstring_samples_large[i] for i in selected_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cb6bfc6-5cae-40ba-a745-1d9a91dc5d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4371\n"
     ]
    }
   ],
   "source": [
    "print(len(code_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307f47d4-ec22-4020-a488-6b064555628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(code_samples[0])\n",
    "print(docstring_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce488786-62e9-44fb-bffb-3224af3d5977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a98cb-f45c-4665-b26d-8bac79c8c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a029a85d-671b-4f20-a45c-93a07fcf1492",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe8c2c-87b4-4822-98cc-114b761cd16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove non-alphanumeric characters\n",
    "    #We may want to keeps some non-alphanumeric characters as they can be crucial to generating certain code\n",
    "    #text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    \n",
    "    # Join tokens back into string\n",
    "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9655f55-9b72-49c5-ba45-4fb8b09ac152",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01631845-80fd-4713-be97-d58f8840ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_cpp(code):\n",
    "    # Remove comments and whitespace (tailored to cpp code)\n",
    "    code = re.sub(r'//.*?\\n|/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "    code = re.sub(r'\\s+', ' ', code)\n",
    "\n",
    "    # Tokenize by splitting on whitespace and symbols\n",
    "    tokens = re.findall(r'[\\w]+|[^\\w\\s]', code)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c766cd92-e72c-4fef-af77-e8ba6805dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_docstrings = [preprocess_text(docstring) for docstring in docstring_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16464590-61ef-4353-b2c1-90fe7c178ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_code_samples = [tokenize_cpp(code) for code in code_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4a44c-3f61-4e28-99a6-544d3b745998",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessed: \", preprocessed_docstrings[0])\n",
    "print(\"Tokenized: \", tokenized_code_samples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77dd524-619e-4e10-a9bd-1774b1f25dcb",
   "metadata": {},
   "source": [
    "Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8a888a-0c9f-48ab-9ab8-2a3890e634f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae79779b-8235-42fd-83b3-b88142494b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer converts text into a matrix of token counts\n",
    "#It tokenizes the text, builds a vocabulary of known words and encodes each text as a vector where each elements represents the count of a word\n",
    "#Fit transform fits the model to the data and transforms the input text data into a sparse matrix representation\n",
    "#TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic that reflects the importance of a \n",
    "#word in a text relatiob to the collection of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed160d2b-31d1-4213-bdd8-6246c862c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature extraction of docstrings\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_counts = count_vectorizer.fit_transform(preprocessed_docstrings)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_tfidf = tfidf_transformer.fit_transform(X_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bdc20f-f325-4baf-86a0-d7c14371e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature extraction of cpp code\n",
    "count_vectorizer_cpp = CountVectorizer()\n",
    "X_counts_cpp = count_vectorizer_cpp.fit_transform(tokenized_code_samples)\n",
    "tfidf_transformer_cpp = TfidfTransformer()\n",
    "X_tfidf_cpp = tfidf_transformer_cpp.fit_transform(X_counts_cpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ccb85-8fd8-4caa-828b-93d7539e79eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of transformed data:\", X_tfidf.shape)\n",
    "print(\"Shape of transformed data:\", X_tfidf_cpp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f16fc8-cfd0-4ca6-b888-b99bc4118349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c2c12-db01-44d0-aa45-490000bf44b8",
   "metadata": {},
   "source": [
    "Decoder-only Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034526f4-7866-4e36-85ae-85638fc67661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Decoder Model\n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(DecoderTransformer, self).__init__()\n",
    "\n",
    "        #Embedding layer converts input tokens into dense vectors\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "        #Adds positional information to input embeddings\n",
    "        self.positional_encoding = PositionalEncoding(hidden_size)\n",
    "\n",
    "        #Transformer decoder layer process the input sequence\n",
    "        decoder_layers = TransformerDecoderLayer(hidden_size, nhead=8)\n",
    "\n",
    "        #Combines multiple decoder layers\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layers, num_layers)\n",
    "\n",
    "        #Output layer transforms decoder ouputs into final output space\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, src, tgt, hidden_size):\n",
    "        #Embedding and positional encoding for target input\n",
    "        tgt = self.embedding(tgt) * math.sqrt(hidden_size)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "\n",
    "        #Processes the target input and source context\n",
    "        output = self.transformer_decoder(tgt, src)\n",
    "\n",
    "        #Final output transformation\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fccd30-e35a-427f-b4a8-18a477c02fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d796950-73da-41dc-bdea-b2766a0c0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty set to store unique tokens\n",
    "vocabulary_set = set()\n",
    "# Add tokens from each tokenized sample to the vocabulary set\n",
    "for tokens in tokenized_code_samples:\n",
    "    vocabulary_set.update(tokens)\n",
    "\n",
    "vocabulary = sorted(list(vocabulary_set))\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9696c9a5-e48e-4c7c-ad76-35c4e6874ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the model\n",
    "input_size = len(vocabulary)  # Size of the vocabulary\n",
    "hidden_size = 32\n",
    "num_layers = 6\n",
    "output_size = len(vocabulary)  # Size of the vocabulary for output\n",
    "model = DecoderTransformer(input_size, hidden_size, num_layers, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a074f2db-67eb-466e-87c0-4284c2d5385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770bc7fa-ff1d-4daa-9b40-4edaf42174ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TF-IDF matrices to tensors\n",
    "#sample_size = 500\n",
    "#X_subset = X_tfidf[:sample_size]\n",
    "#y_subset = X_tfidf_cpp[:sample_size]\n",
    "\n",
    "X_train_tensor = torch.tensor(X_tfidf.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(X_tfidf_cpp.toarray(), dtype=torch.int)\n",
    "\n",
    "seq_length = X_train_tensor.shape[1]\n",
    "\n",
    "X_train_tensor = X_train_tensor.unsqueeze(1).expand(-1, seq_length, -1)\n",
    "#y_train_tensor = y_train_tensor.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bfd030-943f-4b50-b350-5e6f932ae05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Calculate the number of batches\n",
    "num_batches = math.ceil(len(X_train_tensor) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70a61f9-a93e-44ef-a386-183b04da3b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "#keeps breaking here no longer have any time to further debug \n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        # Get batch indices\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(X_train_tensor))\n",
    "\n",
    "        # Extract batch data\n",
    "        batch_X = X_train_tensor[start_idx:end_idx]\n",
    "        batch_y = y_train_tensor[start_idx:end_idx]\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X, batch_y, hidden_size)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print training loss after each epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / num_batches}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbb7580-381d-4926-8455-8a4b20876d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_val_tensor, y_val_tensor)\n",
    "    val_loss = criterion(outputs, y_val_tensor)\n",
    "    print(f'Validation Loss: {val_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc41fd-25d3-4f31-a3b3-e739c92a53d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'trained_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27a8121-30eb-4f3d-99a1-0655e609ba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(description):\n",
    "    generated_code = model.generate_code(description)\n",
    "\n",
    "    return generated_code\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d26599-51e7-4b6b-aec5-3b80a0ce19a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a49e347f-d269-44cc-b18a-fd7e91976be4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     out \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mTextbox(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m btn \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mButton(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubmit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m btn\u001b[38;5;241m.\u001b[39mclick(fn\u001b[38;5;241m=\u001b[39mmodel, inputs\u001b[38;5;241m=\u001b[39minp, outputs\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "with gr.Blocks(title=\"C++ Code Generation\") as server:\n",
    "    gr.Markdown(\"Specifiy the type of code you want to generate\")\n",
    "    with gr.Row():\n",
    "        inp = gr.Textbox(label=\"Description\", placeholder=\"Input Text here\")\n",
    "        out = gr.Textbox(label=\"Code\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    btn.click(fn=generate, inputs=inp, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ded574-19e0-423e-9cbf-8107f74fc87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "server.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
